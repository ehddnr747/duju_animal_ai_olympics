{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class VanillaSACPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr, device):\n",
    "        super(VanillaSACPolicy, self).__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.actor_lr = lr\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 256).to(device)\n",
    "        self.fc2 = nn.Linear(256, 256).to(device)\n",
    "        self.mu = nn.Linear(256, action_dim).to(device)\n",
    "        self.log_std = nn.Linear(256, action_dim).to(device)\n",
    "\n",
    "        nn.init.uniform_(tensor=self.mu.weight, a = -3e-3, b=3e-3)\n",
    "        nn.init.uniform_(tensor=self.mu.bias, a=-3e-3, b=3e-3)\n",
    "\n",
    "        nn.init.uniform_(tensor=self.log_std.weight, a=-3e-3, b=3e-3)\n",
    "        nn.init.uniform_(tensor=self.log_std.bias, a=-3e-3, b=3e-3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        mu = F.relu(self.mu(x))\n",
    "        log_std = F.relu(self.log_std(x))\n",
    "        \n",
    "\n",
    "        # It should return mu, log_std\n",
    "\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample_with_logp(self, x):\n",
    "        mu, log_std = self.forward(x)\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        normal = Normal(mu, std)\n",
    "        x_t = normal.rsample()\n",
    "        logp = normal.log_prob(x_t)\n",
    "\n",
    "        y_t = torch.tanh(x_t)\n",
    "        logp -= torch.log(1 - torch.pow(y_t, 2) + 1e-6)\n",
    "        \n",
    "        return y_t, logp\n",
    "    \n",
    "    def sample(self, x):\n",
    "        mu, log_std = self.forward(x)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        normal = Normal(mu, std)\n",
    "        \n",
    "        return normal.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = VanillaSACPolicy(1,1,1e-4,torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_std = t.forward(torch.FloatTensor([0.0]).to(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = t.sample_with_logp(torch.FloatTensor([0.0]).to(torch.device(\"cuda\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9147], device='cuda:0', grad_fn=<TanhBackward>),\n",
       " tensor([-0.3063], device='cuda:0', grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = torch.sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.fc3.weight.grad[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a = t.forward(torch.FloatTensor([1.0]).to(torch.device(\"cuda\")))\n",
    "    aa = torch.mean(a)\n",
    "    aa.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.forward(torch.FloatTensor([1.0]).to(torch.device(\"cuda\")))\n",
    "aa = torch.mean(a)\n",
    "aa.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t.fc3.weight.grad[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.forward(torch.FloatTensor([1.0]).to(torch.device(\"cuda\")))\n",
    "aa = torch.mean(a, dim=2,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = aa.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.grad_fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
