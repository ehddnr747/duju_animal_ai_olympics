/home/duju/cd2018-2,p3.5/bin/python3 /home/duju/animal_ai_olympics/duju_animal_ai_olympics/tdqn_exp/base_tdqn_conv.py
Conv_Discrete_TDQN_celu_larger networks
reward_compensate 1
skip_frame 1
input channel size :  3
fc input size :  1536
input channel size :  3
fc input size :  1536
Triple_DQN(
  (q1_conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
  (q1_conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
  (q1_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (q1_conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (q1_fc1): Linear(in_features=1536, out_features=256, bias=True)
  (q1_fc2): Linear(in_features=256, out_features=256, bias=True)
  (q1_fc3): Linear(in_features=256, out_features=3, bias=True)
  (q2_conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
  (q2_conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
  (q2_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (q2_conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (q2_fc1): Linear(in_features=1536, out_features=256, bias=True)
  (q2_fc2): Linear(in_features=256, out_features=256, bias=True)
  (q2_fc3): Linear(in_features=256, out_features=3, bias=True)
)
1	74 *** (0.41962775588035583, 0.18913044035434723, 0.7286159992218018, 0.4159875512123108, 0.07241049408912659)
2	139 *** (0.8505200743675232, 0.20714616775512695, 1.893425703048706, 0.8566111922264099, 0.10069315135478973)
3	47 *** (0.9537665247917175, 0.15070396661758423, 3.3915350437164307, 0.939855694770813, 0.07197324931621552)
4	129 *** (1.1510076522827148, -0.17968249320983887, 3.2741851806640625, 1.1466002464294434, 0.07645487785339355)
5	117 *** (2.230175733566284, 0.2800295650959015, 5.772082805633545, 2.206904172897339, 0.11837118864059448)
6	95 *** (2.3363497257232666, 0.3695235550403595, 5.465333938598633, 2.3481929302215576, 0.09514029324054718)
7	159 *** (3.007061004638672, 0.14560872316360474, 7.440912246704102, 3.0796737670898438, 0.13696175813674927)
8	91 *** (3.4207801818847656, 0.07005169987678528, 7.436498165130615, 3.445180892944336, 0.13299866020679474)
9	187 *** (3.1171274185180664, 0.49647849798202515, 8.69690990447998, 3.065108060836792, 0.0964188426733017)
10	79 *** (3.9900386333465576, 0.6855846643447876, 12.48276424407959, 3.917468786239624, 0.10609912872314453)
11	50 *** (3.8234241008758545, 0.1812126785516739, 12.61259937286377, 3.9012436866760254, 0.11849147081375122)
12	136 *** (5.358057975769043, 0.9275389313697815, 14.568818092346191, 5.302915573120117, 0.1733129322528839)
13	118 *** (3.96946382522583, 0.835111141204834, 12.082467079162598, 4.034764766693115, 0.08583908528089523)
14	162 *** (5.1071577072143555, 2.0147202014923096, 12.781707763671875, 5.130492210388184, 0.11089988052845001)
15	201 *** (4.663596153259277, 1.3444361686706543, 12.325614929199219, 4.767323970794678, 0.10225218534469604)
16	64 *** (4.461759567260742, 0.875541627407074, 13.885364532470703, 4.5496439933776855, 0.09418457746505737)
17	135 *** (7.784842491149902, 3.1259474754333496, 20.7734375, 9.904199600219727, 0.09917442500591278)
18	43 *** (5.2604875564575195, 0.2597000002861023, 18.915239334106445, 11.354766845703125, 0.1600622832775116)
19	34 *** (-5.733379364013672, -7.541360855102539, -3.7467234134674072, 9.607898712158203, 0.10971297323703766)
20	0 *** (-5.822810173034668, -8.055924415588379, -3.7491424083709717, 9.053231239318848, 0.07890354096889496)
21	2 *** (-5.76600456237793, -7.586329936981201, -3.015946388244629, 9.052144050598145, 0.07841704785823822)
22	1 *** (-5.749351501464844, -7.597066879272461, -3.6067981719970703, 9.68917465209961, 0.10548165440559387)

