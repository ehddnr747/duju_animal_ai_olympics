{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is for submission ###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def torch_network_load(net, path, device=torch.device(\"cpu\")):\n",
    "    load_dict = torch.load(path,device)\n",
    "    parameters = load_dict[\"model_state_dict\"]\n",
    "    optimizer = load_dict[\"optimizer_state_dict\"]\n",
    "\n",
    "    net.load_state_dict(parameters)\n",
    "    net.optimizer.load_state_dict(optimizer)\n",
    "\n",
    "class Rainbow_DQN_Conv(nn.Module):\n",
    "    def __init__(self, step_size, channel_size, height, width, action_dim, lr, device):\n",
    "        super(Rainbow_DQN_Conv, self).__init__()\n",
    "\n",
    "        self.step_size = step_size\n",
    "        self.channel_size = channel_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "\n",
    "        self.input_channel_size = self.step_size * self.channel_size\n",
    "        self.fc_input_size = 32 * 9\n",
    "\n",
    "        self.uniform_probs = [1.0/self.action_dim] * self.action_dim\n",
    "\n",
    "        # Q1\n",
    "        self.q1_conv1 = nn.Conv2d(in_channels=self.input_channel_size, out_channels=32,\n",
    "                                  kernel_size=4, stride=2, padding=0).to(device)  # [32, 41, 41]\n",
    "        self.q1_conv2 = nn.Conv2d(in_channels=32, out_channels=32,\n",
    "                                  kernel_size=5, stride=2, padding=2).to(device)  # [32, 21, 21]\n",
    "        self.q1_conv3 = nn.Conv2d(in_channels=32, out_channels=32,\n",
    "                                  kernel_size=7, stride=7, padding=0).to(device)  # [32, 3, 3]\n",
    "        # self.q1_conv4 = nn.Conv2d(in_channels=64, out_channels=128,\n",
    "        #                           kernel_size=3, stride=1, padding=0).to(device)  # [128, 1, 1]\n",
    "\n",
    "        self.q1_fc1 = nn.Linear(self.fc_input_size, 128).to(device)\n",
    "        self.q1_fc2_V = nn.Linear(128, 128).to(device)\n",
    "        self.q1_fc2_A = nn.Linear(128, 128).to(device)\n",
    "\n",
    "        self.q1_V = nn.Linear(128, 1).to(device)\n",
    "        self.q1_A = nn.Linear(128, action_dim).to(device)\n",
    "\n",
    "        # Q2\n",
    "        self.q2_conv1 = nn.Conv2d(in_channels=self.input_channel_size, out_channels=32,\n",
    "                                  kernel_size=4, stride=2, padding=0).to(device)  # [32, 41, 41]\n",
    "        self.q2_conv2 = nn.Conv2d(in_channels=32, out_channels=32,\n",
    "                                  kernel_size=5, stride=2, padding=2).to(device)  # [32, 21, 21]\n",
    "        self.q2_conv3 = nn.Conv2d(in_channels=32, out_channels=32,\n",
    "                                  kernel_size=7, stride=7, padding=0).to(device)  # [64, 3, 3]\n",
    "        # self.q2_conv4 = nn.Conv2d(in_channels=64, out_channels=128,\n",
    "        #                           kernel_size=3, stride=1, padding=0).to(device)  # [128, 1, 1]\n",
    "\n",
    "        self.q2_fc1 = nn.Linear(self.fc_input_size, 128).to(device)\n",
    "        self.q2_fc2_V = nn.Linear(128, 128).to(device)\n",
    "        self.q2_fc2_A = nn.Linear(128, 128).to(device)\n",
    "\n",
    "        self.q2_V = nn.Linear(128, 1).to(device)\n",
    "        self.q2_A = nn.Linear(128, action_dim).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr, weight_decay=1e-6)\n",
    "\n",
    "        self.milestones = [100000000]\n",
    "        self.step_lr_scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=self.milestones,gamma=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4\n",
    "\n",
    "        x1 = F.celu(self.q1_conv1(x))\n",
    "        x1 = F.celu(self.q1_conv2(x1))\n",
    "        x1 = F.celu(self.q1_conv3(x1))\n",
    "        #x1 = F.celu(self.q1_conv4(x1))\n",
    "\n",
    "        x1 = x1.view(-1, self.fc_input_size)\n",
    "\n",
    "        x1 = F.celu(self.q1_fc1(x1))\n",
    "        x1_V = F.celu(self.q1_fc2_V(x1))\n",
    "        x1_A = F.celu(self.q1_fc2_A(x1))\n",
    "\n",
    "        x1_V = self.q1_V(x1_V)\n",
    "        x1_A = self.q1_A(x1_A)\n",
    "\n",
    "        x1_A_mean = torch.mean(x1_A, dim=1,keepdim=True)\n",
    "        x1_A_mean = torch.cat([x1_A_mean]*self.action_dim, dim=1)\n",
    "\n",
    "        x1_A = x1_A - x1_A_mean\n",
    "\n",
    "        x1 = x1_V + x1_A\n",
    "\n",
    "        x2 = F.celu(self.q2_conv1(x))\n",
    "        x2 = F.celu(self.q2_conv2(x2))\n",
    "        x2 = F.celu(self.q2_conv3(x2))\n",
    "        #x2 = F.celu(self.q2_conv4(x2))\n",
    "\n",
    "        x2 = x2.view(-1, self.fc_input_size)\n",
    "\n",
    "        x2 = F.celu(self.q2_fc1(x2))\n",
    "        x2_V = F.celu(self.q2_fc2_V(x2))\n",
    "        x2_A = F.celu(self.q2_fc2_A(x2))\n",
    "\n",
    "        x2_V = self.q2_V(x2_V)\n",
    "        x2_A = self.q2_A(x2_A)\n",
    "\n",
    "        x2_A_mean = torch.mean(x2_A, dim=1, keepdim=True)\n",
    "        x2_A_mean = torch.cat([x2_A_mean] * self.action_dim, dim=1)\n",
    "\n",
    "        x2_A = x2_A - x2_A_mean\n",
    "\n",
    "        x2 = x2_V + x2_A\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def epsilon_sample(self, x, epsilon):\n",
    "        exploration_flag = bool(binomial(1, epsilon))\n",
    "\n",
    "        if exploration_flag:\n",
    "            action = np.argmax(multinomial(1, self.uniform_probs))\n",
    "            return action\n",
    "\n",
    "        else:\n",
    "            q1, q2 = self.forward(x)\n",
    "            Q = q1 + q2\n",
    "\n",
    "            action = torch.argmax(Q, dim=1)\n",
    "            assert action.shape == (1,)\n",
    "\n",
    "            return action.detach().cpu().numpy()[0]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ImageBuffer(object):\n",
    "\n",
    "    # idx starts from 1. The first image will have idx 1.\n",
    "    # When the buffer get full with batch size 100, then there will be 100 iamges and the current idx will be 100.\n",
    "\n",
    "    def __init__(self, height, width, stepsize, channel_size, buffer_size):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.stepsize = stepsize\n",
    "        self.channel_size = channel_size\n",
    "        self.step_channelsize = stepsize * channel_size\n",
    "\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "\n",
    "        assert self.stepsize <= self.buffer_size\n",
    "\n",
    "        self.full_count = 0\n",
    "\n",
    "\n",
    "    # # [height, width, channel]\n",
    "    # def dm_add(self, frame):\n",
    "    #     frame = np.moveaxis(frame, [0, 1, 2], [1, 2, 0])\n",
    "    #     frame = (frame / 128.0) - 1.0\n",
    "    #\n",
    "    #     if self.count < self.buffer_size:\n",
    "    #         self.count += 1\n",
    "    #         self.buffer.append(frame)\n",
    "    #     else:\n",
    "    #         self.buffer.pop(0)\n",
    "    #         self.buffer.append(frame)\n",
    "    #         self.full_count += 1\n",
    "\n",
    "    # [height, width, channel]\n",
    "    def dm_add_gray(self, frame):\n",
    "        frame = frame / 256.0\n",
    "        frame = frame[:, :, [0]] * 0.2989 + frame[:, :, [1]] * 0.5870 + frame[:, :, [2]] * 0.1140\n",
    "        frame = np.moveaxis(frame, [0, 1, 2], [1, 2, 0]) # [1, height, width]\n",
    "\n",
    "        assert frame.shape == (1, self.height, self.width)\n",
    "\n",
    "        if self.count < self.buffer_size:\n",
    "            self.count += 1\n",
    "            self.buffer.append(frame)\n",
    "        else:\n",
    "            self.buffer.pop(0)\n",
    "            self.buffer.append(frame)\n",
    "            self.full_count += 1\n",
    "\n",
    "    def animal_add(self, frame):\n",
    "\n",
    "        #store as np.ubyte\n",
    "\n",
    "        frame = np.moveaxis(frame, [0, 1, 2], [1, 2, 0]) * 256\n",
    "        frame = np.array(frame, dtype=np.ubyte)\n",
    "\n",
    "        assert frame.shape == (self.channel_size, self.height, self.width)\n",
    "\n",
    "        if self.count < self.buffer_size:\n",
    "            self.count += 1\n",
    "            self.buffer.append(frame)\n",
    "        else:\n",
    "            self.buffer.pop(0)\n",
    "            self.buffer.append(frame)\n",
    "            self.full_count += 1\n",
    "\n",
    "\n",
    "    def get_state(self, idx):\n",
    "        assert idx > self.full_count\n",
    "        assert idx <= self.count + self.full_count\n",
    "\n",
    "        if self.count < self.buffer_size:\n",
    "            temp_idx = idx\n",
    "        else:\n",
    "            temp_idx = idx - self.full_count\n",
    "\n",
    "        return_array = np.concatenate(self.buffer[temp_idx - self.stepsize: temp_idx], axis=0)\n",
    "        # because image idx starts from 1 and list idx starts from 0\n",
    "\n",
    "        assert return_array.shape == (self.step_channelsize, self.height, self.width)\n",
    "\n",
    "        return return_array / np.array(256, dtype=np.float32)\n",
    "        #[stepsize, height, width]\n",
    "\n",
    "    def get_state_and_next(self, idx):\n",
    "\n",
    "        assert idx > 0 and idx < self.count + self.full_count\n",
    "\n",
    "        return self.get_state(idx), self.get_state(idx+1)\n",
    "\n",
    "    def get_current_index(self):\n",
    "        return self.count + self.full_count\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.action_dict = { 0 : np.array([1,0]), # forward\n",
    "                1 : np.array([0,2]), # left\n",
    "                2 : np.array([0,1]), # right\n",
    "                        }\n",
    "        self.net = Rainbow_DQN_Conv(4, 3, 84, 84, 3, 3e-5, torch.device(\"cpu\"))\n",
    "        self.image_buffer = ImageBuffer(84, 84, 4, 3, 2000)\n",
    "        self.start_flag = True\n",
    "        \n",
    "        \n",
    "\n",
    "    def reset(self, t=250):\n",
    "        \"\"\"\n",
    "        Reset is called before each episode begins\n",
    "        Leave blank if nothing needs to happen there\n",
    "        :param t the number of timesteps in the episode\n",
    "        \"\"\"\n",
    "        self.start_flag = True\n",
    "\n",
    "    def step(self, obs, reward, done, info):\n",
    "        \"\"\"\n",
    "        :param obs: agent's observation of the current environment\n",
    "        :param reward: amount of reward returned after previous action\n",
    "        :param done: whether the episode has ended.\n",
    "        :param info: contains auxiliary diagnostic information, including BrainInfo.\n",
    "        :return: the action to take, a list or size 2\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.start_flag:\n",
    "            self.start_flag = False\n",
    "            \n",
    "            initial_frame = info.visual_observations[0][0] #[height, width, channel]\n",
    "            \n",
    "            for _ in range(4):\n",
    "                image_buffer.animal_add(initial_frame)\n",
    "            \n",
    "            s_idx = image_buffer.get_current_index()\n",
    "            input_state = image_buffer.get_state(s_idx)\n",
    "        \n",
    "        else:\n",
    "            s_frame = info.visual_observations[0][0] #[height, width, channel]\n",
    "            image_buffer.animal_add(s_frame)\n",
    "            s_idx = image_buffer.get_current_index()\n",
    "            input_state = image_buffer.get_state(s_idx)\n",
    "        \n",
    "        a_category = self.net.epsilon_sample(\n",
    "            torch.FloatTensor(input_state).view(1, 4 * 3, 84, 84),\n",
    "            0.0 # epsilon\n",
    "            )\n",
    "            \n",
    "        action = self.action_dict[a_category]\n",
    "        \n",
    "        return action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
